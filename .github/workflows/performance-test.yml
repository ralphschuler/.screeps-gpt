---
name: Performance Benchmark

on:
  pull_request:
    branches: [main]
    paths:
      - "packages/bot/src/runtime/**"
      - "tests/performance/**"
      - ".github/workflows/performance-test.yml"
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: performance-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm install --legacy-peer-deps

      - name: Build bot
        run: npm run build

      - name: Start private Screeps server
        run: |
          docker-compose -f docker-compose.test.yml up \
            -d screeps-test-server
          echo "â³ Waiting for Screeps server to be ready..."

      - name: Wait for server ready
        timeout-minutes: 3
        run: |
          attempt=0
          max_attempts=36

          while [ $attempt -lt $max_attempts ]; do
            if curl --fail --silent \
              http://localhost:21025/api/version \
              > /dev/null 2>&1; then
              echo "âœ“ Screeps server is ready!"
              exit 0
            fi

            echo "Attempt $((attempt + 1))/$max_attempts: \
          Server not ready yet..."
            sleep 5
            attempt=$((attempt + 1))
          done

          echo "âŒ Server failed to start within timeout"
          docker-compose -f docker-compose.test.yml logs \
            screeps-test-server
          exit 1

      - name: Run performance tests
        id: performance_tests
        continue-on-error: true
        run: |
          npm run test:performance 2>&1 | \
            tee performance-test-output.log
          echo "test_exit_code=$?" >> $GITHUB_OUTPUT

      - name: Stop Screeps server
        if: always()
        run: docker-compose -f docker-compose.test.yml down

      - name: Generate performance report
        if: always()
        run: |
          mkdir -p reports/performance

          # Set status based on test result
          EXIT_CODE="${{ steps.performance_tests.outputs.test_exit_code }}"
          if [ "${EXIT_CODE}" = "0" ]; then
            STATUS="Passed âœ…"
          else
            STATUS="Skipped âš ï¸"
          fi

          # Create summary report
          cat > reports/performance/summary.md <<EOF
          ## ðŸŽ¯ Performance Benchmark Results

          Status: ${STATUS}

          **Build:** #${{ github.run_number }}
          **Commit:** \`${{ github.sha }}\`
          **Branch:** \`${{ github.head_ref || github.ref_name }}\`

          ### Test Configuration

          - **Max Ticks:** 10,000
          - **Check Interval:** 100 ticks
          - **Speedrun Mode:** Enabled
          - **Server:** Private Screeps server (Docker)

          ### Notes

          Performance tests require a running private Screeps server
          with:
          - MongoDB for game state
          - Redis for caching
          - Bot opponents deployed via screepsmod-bots

          Tests are currently marked as .skip() and require manual
          server setup.
          Future work will enable full CI integration with automated
          bot deployment.

          ### Next Steps

          1. Review test output in artifacts
          2. Compare metrics against baseline if available
          3. Update baseline after verified improvements

          ---

          ðŸ“Š Full logs available in workflow artifacts
          EOF

      - name: Upload performance report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: |
            reports/performance/
            performance-test-output.log
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            let reportContent =
              '## ðŸŽ¯ Performance Benchmark Results\n\n';

            try {
              // Read summary if it exists
              if (fs.existsSync('reports/performance/summary.md')) {
                reportContent = fs.readFileSync(
                  'reports/performance/summary.md', 'utf8');
              } else {
                reportContent += '**Status:** Tests completed\n\n';
                reportContent +=
                  'Performance benchmark executed. ' +
                  'See artifacts for detailed results.\n';
              }
            } catch (error) {
              reportContent +=
                'âš ï¸ Unable to read performance report\n\n';
              reportContent += 'Check workflow logs for details.\n';
            }

            reportContent += '\n\n---\n';
            reportContent +=
              '_Performance testing infrastructure is under ' +
              'development._\n';
            reportContent +=
              '_See [docs/testing/private-server.md]' +
              '(https://github.com/${{ github.repository }}/blob/' +
              '${{ github.sha }}/docs/testing/private-server.md) ' +
              'for details._';

            // Find existing comment
            const { data: comments } =
              await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            // Create or update comment
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: reportContent
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: reportContent
              });
            }
